{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {    "tags": [
    "remove-cell"
]},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "InceptionV3 est un modèle de réseau neuronal convolutif (CNN) qui a été formé sur plus d’un million d’images de la base de données ImageNet. Il est conçu par Google et décrit dans le document ‘Rethinking the Inception Architecture for Computer Vision’.\n",
    "\n",
    "## Les concepts d'InceptionV3\n",
    "\n",
    "*Architecture Inception* : L’idée principale derrière l’architecture Inception est de trouver comment un réseau de neurones optimal peut être construit de manière automatique. Pour ce faire, Inception utilise une approche modulaire, où un bloc Inception (ou module) est créé et répété dans le réseau. Chaque bloc Inception contient plusieurs filtres de convolution de différentes tailles qui sont exécutés en parallèle.\n",
    "\n",
    "\n",
    "*Facteur de normalisation* : InceptionV3 utilise la normalisation par lots, qui est une technique pour améliorer la vitesse, les performances et la stabilité des réseaux de neurones artificiels. Elle normalise les couches d’entrée en réajustant et en redimensionnant les activations.\n",
    "\n",
    "\n",
    "*Convolution asymétrique* : Pour réduire le nombre de paramètres et le coût de calcul, InceptionV3 utilise une convolution asymétrique, c’est-à-dire qu’elle remplace les grands filtres de convolution (par exemple, 5x5) par deux filtres de convolution plus petits (3x3 puis 3x1).\n",
    "\n",
    "\n",
    "*Décomposition en profondeur* : InceptionV3 décompose les filtres de convolution 7x7 en filtres 3x3. Cela réduit le nombre de paramètres et améliore l’efficacité du réseau.\n",
    "Global Average Pooling : À la fin du réseau, une couche de Global Average Pooling est utilisée à la place des couches entièrement connectées. Cela réduit considérablement le nombre de paramètres dans le réseau et aide à lutter contre le surapprentissage.\n",
    "\n",
    "\n",
    "*Softmax* : Enfin, une couche softmax est utilisée pour produire une distribution de probabilité pour les prédictions de classe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step\n",
      "Prédiction : ('sports_car', 0.6899679)\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "# Téléchargement du modèle InceptionV3 pré-entraîné\n",
    "model = InceptionV3(weights='imagenet')\n",
    "\n",
    "# Fonction pour prédire la classe d'une image\n",
    "def predict_class(image_path):\n",
    "    img = image.load_img(image_path, target_size=(299, 299))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    preds = model.predict(x)\n",
    "    print('Prédiction :', decode_predictions(preds, top=1)[0][0][1:3])\n",
    "\n",
    "\n",
    "predict_class('../assets/car.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/miniconda3/envs/ai-explorer/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/ben/miniconda3/envs/ai-explorer/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédiction : sports car 80.13818359375\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Chargement des noms de classes ImageNet\n",
    "with open('../assets/imagenet_classes.txt') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Téléchargement du modèle InceptionV3 pré-entraîné\n",
    "model = models.inception_v3(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Définition de la transformation de l'image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Fonction pour prédire la classe d'une image\n",
    "def predict_class(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img_t = transform(img)\n",
    "    batch_t = torch.unsqueeze(img_t, 0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(batch_t)\n",
    "    _, indices = torch.max(out, 1)\n",
    "    percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
    "    print('Prédiction :', classes[indices[0]], percentage[indices[0]].item())\n",
    "\n",
    "# Test de la fonction de prédiction\n",
    "predict_class('../assets/car.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quand utiliser le modele Inception ? \n",
    "\n",
    "*Classification d’images* : Inception est un excellent choix pour les tâches de classification d’images en raison de sa précision élevée. Il a été formé sur la base de données ImageNet, qui contient plus d’un million d’images de 1000 classes différentes.\n",
    "\n",
    "\n",
    "*Transfert d’apprentissage* : Inception est souvent utilisé comme point de départ pour le transfert d’apprentissage. Le transfert d’apprentissage est une technique où un modèle pré-entraîné est utilisé sur un nouveau problème similaire. Il est particulièrement utile lorsque vous disposez de peu de données pour votre problème spécifique.\n",
    "\n",
    "\n",
    "*Extraction de caractéristiques* : Les modèles Inception peuvent être utilisés pour extraire des caractéristiques utiles à partir d’images. Ces caractéristiques peuvent ensuite être utilisées pour former d’autres modèles ou pour effectuer d’autres tâches, comme la recherche d’images similaires.\n",
    "\n",
    "*Fine-tuning* : Si vous avez suffisamment de données et de ressources de calcul, vous pouvez effectuer un fine-tuning du modèle Inception sur vos propres données. Cela peut améliorer les performances si vos données sont suffisamment différentes de celles sur lesquelles Inception a été initialement formé.\n",
    "\n",
    "\n",
    "Il est important de noter que bien que Inception soit un modèle puissant, il n’est pas toujours le meilleur choix pour chaque tâche. Par exemple, pour les tâches impliquant des images de très haute résolution, des modèles avec une architecture différente peuvent être plus appropriés. Il peut egalement être trop complexe et coûteux en calcul pour certaines applications, en particulier celles qui nécessitent une latence faible ou qui sont déployées sur des appareils avec des ressources limitées. Dans ces cas, des modèles plus petits et plus efficaces, comme MobileNet, peuvent être une meilleure option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-explorer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
