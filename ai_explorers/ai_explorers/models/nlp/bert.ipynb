{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT, qui signifie Bidirectional Encoder Representations from Transformers, est un modèle de traitement du langage naturel développé par Google. Il est conçu pour pré-entraîner des représentations profondes en prenant en compte le contexte bidirectionnel complet d’un mot (c’est-à-dire, les mots à gauche et à droite)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Détails techniques de BERT\n",
    "\n",
    "BERT est basé sur l’architecture des Transformers. Les Transformers sont des modèles qui utilisent des mécanismes d’attention pour comprendre le contexte d’un mot dans une phrase.\n",
    "\n",
    "\n",
    "Contrairement aux modèles précédents qui analysaient les phrases de gauche à droite ou de droite à gauche, BERT est bidirectionnel. Cela signifie qu’il examine le contexte des mots avant et après un mot donné. Cela permet à BERT de comprendre le contexte et l’intention derrière les mots d’une manière plus précise.\n",
    "\n",
    "\n",
    "BERT est pré-entraîné sur un grand corpus de texte (Wikipedia et BooksCorpus) en utilisant deux tâches : la prédiction du masque de langue et la prédiction de la phrase suivante.\n",
    "\n",
    "### Mecanisme d'attention\n",
    "\n",
    "Le mécanisme d’attention est une partie clé des modèles Transformer. Il permet au modèle de se concentrer sur différentes parties d’une phrase lorsqu’il génère une représentation pour un mot spécifique. En termes simples, l’attention est un score qui détermine à quel point chaque mot dans une phrase est pertinent pour chaque autre mot. Il est calculé en utilisant les produits scalaires des vecteurs d’embedding des mots, qui sont ensuite passés par une fonction softmax pour obtenir des valeurs de probabilité.\n",
    "\n",
    "### Transformer\n",
    "\n",
    "Un Transformer est un type de modèle de traitement du langage naturel qui a été introduit dans le papier “Attention is All You Need” par Vaswani et al. en 2017. Il a été conçu pour résoudre le problème des séquences longues dans les modèles de réseau de neurones récurrents (RNN) et pour améliorer la vitesse d’entraînement.\n",
    "\n",
    "Il est composé des elements suivants:\n",
    "\n",
    "\n",
    "*Mécanisme d’attention*: Explication juste au dessus\n",
    "\n",
    "*Encodage positionnel* : Les Transformers utilisent un encodage positionnel pour tenir compte de l’ordre des mots dans une phrase. Cela permet au modèle de distinguer les phrases qui ont les mêmes mots mais dans un ordre différent.\n",
    "\n",
    "\n",
    "*Architecture d’encodeur-décodeur* : Les Transformers sont composés d’un encodeur qui transforme une phrase d’entrée en une représentation intermédiaire, et d’un décodeur qui génère une phrase de sortie à partir de cette représentation.\n",
    "\n",
    "*Parallélisation* : Contrairement aux RNN qui traitent les mots de manière séquentielle, les Transformers peuvent traiter tous les mots d’une phrase en parallèle, ce qui permet d’accélérer l’entraînement.\n",
    "\n",
    "\n",
    "### Prédiction du masque de langue (Masked Language Model, MLM)\n",
    "\n",
    "C’est l’une des deux tâches d’entraînement utilisées pour pré-entraîner BERT. Dans MLM, environ 15% des mots d’une phrase sont masqués (c’est-à-dire, remplacés par un token spécial [MASK]), et le modèle doit prédire ces mots masqués en se basant sur le contexte fourni par les mots non masqués. Cela permet au modèle d’apprendre une représentation riche du langage qui tient compte du contexte des mots.\n",
    "\n",
    "\n",
    "### Prédiction de la phrase suivante (Next Sentence Prediction, NSP) \n",
    "\n",
    "C’est la deuxième tâche d’entraînement utilisée pour pré-entraîner BERT. Dans NSP, le modèle reçoit deux phrases et doit prédire si la deuxième phrase est la suite logique de la première. Cela aide le modèle à comprendre les relations entre les phrases, ce qui est crucial pour de nombreuses tâches de traitement du langage naturel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {    "tags": [
        "remove-cell"
    ]},
   "source": [
    "## Exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/miniconda3/envs/ai-explorer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1144,  0.1937,  0.1250,  ..., -0.3827,  0.2107,  0.5407],\n",
       "         [ 0.5308,  0.3207,  0.3665,  ..., -0.0036,  0.7579,  0.0388],\n",
       "         [-0.4877,  0.8849,  0.4256,  ..., -0.6976,  0.4458,  0.1231],\n",
       "         ...,\n",
       "         [-0.7003, -0.1815,  0.3297,  ..., -0.4838,  0.0680,  0.8901],\n",
       "         [-1.0355, -0.2567, -0.0317,  ...,  0.3197,  0.3999,  0.1795],\n",
       "         [ 0.6080,  0.2610, -0.3131,  ...,  0.0311, -0.6283, -0.1994]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Étape 2: Initialisation du tokenizer et du modèle\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Étape 3: Préparation de l'entrée\n",
    "text = \"Hello, my dog is cute\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Étape 4: Passage de l'entrée dans le modèle\n",
    "output = model(**encoded_input)\n",
    "\n",
    "# Étape 5: Utilisation de la sortie\n",
    "# contient les représentations vectorielles de chaque mot dans la phrase d’entrée. \n",
    "# Chaque mot est représenté par un vecteur de 768 dimensions (pour BERT-base)\n",
    "# Ce vecteur capture le contexte du mot dans la phrase.\n",
    "last_hidden_states = output.last_hidden_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quand utiliser BERT?\n",
    "\n",
    "BERT peut être utilisé pour une variété de tâches de traitement du langage naturel, y compris mais sans s’y limiter:\n",
    "\n",
    "*Classification de texte*: Par exemple, catégoriser les articles de blog par sujet.\n",
    "Reconnaissance d’entités nommées: Par exemple, identifier les noms de personnes, les lieux ou les organisations dans un texte.\n",
    "\n",
    "\n",
    "\n",
    "*Chatbot*: Par exemple, créer un système qui répond aux questions posées en langage naturel.\n",
    "\n",
    "\n",
    "\n",
    "*Analyse des sentiments*: Par exemple, comprendre si un commentaire sur un produit est positif ou négatif.\n",
    "\n",
    "\n",
    "\n",
    "BERT est particulièrement utile lorsque vous avez besoin de comprendre le contexte d’un mot dans une phrase ou lorsque vous travaillez avec un grand corpus de texte non structuré. Cependant, il est important de noter que BERT est un modèle assez complexe et nécessite beaucoup de ressources de calcul, donc il peut ne pas être le meilleur choix pour toutes les situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-explorer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
